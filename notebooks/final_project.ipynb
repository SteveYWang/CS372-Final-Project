{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71eca4a6-c279-4729-a280-c85beb254974",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Function and class definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacda544-10d2-4119-80de-1715730f6896",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Import libraries and models\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e651baa7-c6cb-49ba-ae8d-7aeac2d50368",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Dataset class definition\n",
    "'''\n",
    "\n",
    "class PastureDataset(Dataset):\n",
    "    def __init__(self, csv_path, image_root=\".\", transform=None):\n",
    "        self.csv_path = Path(csv_path)\n",
    "        self.image_root = Path(image_root)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Read CSV\n",
    "        df = pd.read_csv(self.csv_path)\n",
    "\n",
    "        required_cols = [\n",
    "            \"image_path\",\n",
    "            \"target\",\n",
    "            \"State\",\n",
    "            \"Species\",\n",
    "            \"Pre_GSHH_NDVI\",\n",
    "            \"Height_Ave_cm\",\n",
    "        ]\n",
    "        missing = [c for c in required_cols if c not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"CSV missing required columns: {missing}\")\n",
    "\n",
    "        # ---- GROUP TARGETS INTO VECTORS ----\n",
    "        targets = (\n",
    "            df.groupby(\"image_path\")[\"target\"]\n",
    "              .apply(lambda x: x.values.astype(np.float32))\n",
    "              .reset_index(name=\"target_vec\")\n",
    "        )\n",
    "\n",
    "        # ---- GET METADATA PER IMAGE (FIRST ROW) ----\n",
    "        meta = (\n",
    "            df.drop_duplicates(\"image_path\")[\n",
    "                [\"image_path\", \"State\", \"Species\", \"Pre_GSHH_NDVI\", \"Height_Ave_cm\"]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Merge targets and metadata\n",
    "        grouped = targets.merge(meta, on=\"image_path\", how=\"left\")\n",
    "\n",
    "        # Sanity check: each image should have exactly 5 target values\n",
    "        bad_rows = grouped[grouped[\"target_vec\"].apply(len) != 5]\n",
    "        if len(bad_rows) > 0:\n",
    "            raise ValueError(\n",
    "                \"Some images do not have exactly 5 target rows:\\n\"\n",
    "                f\"{bad_rows[['image_path', 'target_vec']].head()}\"\n",
    "            )\n",
    "\n",
    "        self.groups = grouped\n",
    "\n",
    "        # Build vocabularies for State and Species and store mappings\n",
    "        states = sorted(self.groups[\"State\"].unique())\n",
    "        species = sorted(self.groups[\"Species\"].unique())\n",
    "\n",
    "        self.state_to_idx = {s: i for i, s in enumerate(states)}\n",
    "        self.idx_to_state = states\n",
    "\n",
    "        self.species_to_idx = {s: i for i, s in enumerate(species)}\n",
    "        self.idx_to_species = species\n",
    "\n",
    "        self.num_states = len(states)\n",
    "        self.num_species = len(species)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.groups)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.groups.iloc[idx]\n",
    "\n",
    "        # Load image\n",
    "        img_path = self.image_root / row[\"image_path\"]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # One-hot encode State\n",
    "        state_idx = self.state_to_idx[row[\"State\"]]\n",
    "        state_one_hot = torch.zeros(self.num_states, dtype=torch.float32)\n",
    "        state_one_hot[state_idx] = 1.0\n",
    "\n",
    "        # One-hot encode Species\n",
    "        species_idx = self.species_to_idx[row[\"Species\"]]\n",
    "        species_one_hot = torch.zeros(self.num_species, dtype=torch.float32)\n",
    "        species_one_hot[species_idx] = 1.0\n",
    "\n",
    "        # Numeric metadata\n",
    "        ndvi = torch.tensor(row[\"Pre_GSHH_NDVI\"], dtype=torch.float32)\n",
    "        height = torch.tensor(row[\"Height_Ave_cm\"], dtype=torch.float32)\n",
    "\n",
    "        # Targets → torch vector of size (5,)\n",
    "        target_vec = torch.tensor(row[\"target_vec\"], dtype=torch.float32)\n",
    "\n",
    "        return img, state_one_hot, species_one_hot, ndvi, height, target_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dbe83e-3920-4eee-9713-a1b0dd88f087",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Model definition\n",
    "'''\n",
    "\n",
    "class BiomassModel(nn.Module):\n",
    "    def __init__(self, drop_percent=0.5):\n",
    "        super().__init__()\n",
    "        resnet_output_dim = 512\n",
    "        num_states = 4\n",
    "        num_species = 15\n",
    "        \n",
    "        #resnet\n",
    "        self.resnet = models.resnet18(weights='DEFAULT')\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        \n",
    "        #final layers\n",
    "        input_dim = resnet_output_dim + num_states + num_species + 2 #plus 2 for height and NDVI\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=drop_percent),\n",
    "            nn.Linear(256, 5)   \n",
    "        )\n",
    "    \n",
    "    def forward(self, images, states, species, ndvis, heights):\n",
    "        #fix dimensions\n",
    "        ndvis = ndvis.unsqueeze(1)\n",
    "        heights = heights.unsqueeze(1)\n",
    "        \n",
    "        #apply resnet on images\n",
    "        resnet_out = self.resnet(images)\n",
    "        \n",
    "        #concatenate resnet output with the non-image inputs\n",
    "        x = torch.cat([resnet_out, states, species, ndvis, heights], dim=1)\n",
    "        \n",
    "        #apply final neural network\n",
    "        out = self.fc(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c1641a-dbce-4c03-9cbd-d82ee5a6c366",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Training function\n",
    "'''\n",
    "\n",
    "def train_model(model, train_loader, val_loader, final_layer_only=True, patience=3, max_epochs=24):\n",
    "    \n",
    "    # ------------------------------------- Model setup --------------------------------------------\n",
    "\n",
    "    #freeze resnet parameters if requested\n",
    "    for parameter in model.resnet.parameters():\n",
    "        if final_layer_only:\n",
    "            parameter.requires_grad = False \n",
    "        else:\n",
    "            parameter.requires_grad = True\n",
    "\n",
    "\n",
    "    #set up device\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    print(device)\n",
    "    model.to(device)\n",
    "\n",
    "    #set up loss and optimizer\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    if final_layer_only:\n",
    "        optimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-3)\n",
    "        #optimizer = torch.optim.SGD(model.fc.parameters(), lr=1e-3)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "        #optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "    # ------------------------------------- Training parameters -----------------------------------\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    no_improvement_count = 0\n",
    "    val_losses = []\n",
    "    best_params = model.state_dict().copy()\n",
    "\n",
    "\n",
    "    # ----------------------------------------- Training loop ---------------------------------------\n",
    "\n",
    "    epoch_count = 0\n",
    "    while epoch_count < max_epochs and no_improvement_count < patience:\n",
    "        model.train()\n",
    "\n",
    "        for train_batch in train_loader:  \n",
    "            #unpacking\n",
    "            images, states, species, ndvis, heights, targets = train_batch\n",
    "\n",
    "            #move batch data to gpu\n",
    "            images = images.to(device)\n",
    "            states = states.to(device)\n",
    "            species = species.to(device)\n",
    "            ndvis = ndvis.to(device)\n",
    "            heights = heights.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            #forward propogation\n",
    "            preds = model(images, states, species, ndvis, heights)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            #backward propogation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            #gradient clip\n",
    "            if final_layer_only:\n",
    "                torch.nn.utils.clip_grad_norm_(model.fc.parameters(), 1.0)\n",
    "            else:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "                \n",
    "            #gradient step\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            #print train loss\n",
    "            print(\"Batch train Loss: \", loss.item())\n",
    "\n",
    "\n",
    "        #validation \n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_batch in val_loader:\n",
    "                #unpacking\n",
    "                val_images, val_states, val_species, val_ndvis, val_heights, val_targets = val_batch\n",
    "\n",
    "                #move batch data to gpu\n",
    "                val_images = val_images.to(device)\n",
    "                val_states = val_states.to(device)\n",
    "                val_species = val_species.to(device)\n",
    "                val_ndvis = val_ndvis.to(device)\n",
    "                val_heights = val_heights.to(device)\n",
    "                val_targets = val_targets.to(device)\n",
    "\n",
    "                #forward prop and analyze\n",
    "                val_preds = model(val_images, val_states, val_species, val_ndvis, val_heights)\n",
    "                val_loss = loss_fn(val_preds, val_targets)\n",
    "                running_val_loss += val_loss.item() * val_images.size(0)\n",
    "\n",
    "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        print(\"Epoch val loss: \", epoch_val_loss, \"\\n\")\n",
    "\n",
    "\n",
    "        #early stopping\n",
    "        if best_val_loss - epoch_val_loss < 0:\n",
    "            no_improvement_count += 1\n",
    "        else:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_params = model.state_dict().copy()\n",
    "            no_improvement_count = 0\n",
    "\n",
    "        epoch_count += 1\n",
    "\n",
    "    #plot loss and accuracy\n",
    "    plot_training_curves(val_losses)\n",
    "\n",
    "    #load best parameters\n",
    "    model.load_state_dict(best_params)\n",
    "    print(\"Best val loss: \", best_val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681db6cc-cb4e-40b8-822b-81bc883bb996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Test evaluation function\n",
    "'''\n",
    "\n",
    "def test(model, test_loader):\n",
    "    #set up device\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    print(device)\n",
    "    model.to(device)\n",
    "    \n",
    "    running_loss = 0\n",
    "    batch_count = 0\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    for batch in test_loader:  \n",
    "            #unpacking\n",
    "            images, states, species, ndvis, heights, targets = batch\n",
    "\n",
    "            #move batch data to gpu\n",
    "            images = images.to(device)\n",
    "            states = states.to(device)\n",
    "            species = species.to(device)\n",
    "            ndvis = ndvis.to(device)\n",
    "            heights = heights.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            #forward propogation\n",
    "            preds = model(images, states, species, ndvis, heights)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            #print train loss\n",
    "            print(\"Batch test loss: \", loss.item())\n",
    "            running_loss += loss.item()\n",
    "            batch_count += 1\n",
    "        \n",
    "    print(\"Avg batch test loss: \", running_loss/batch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3065608-3c63-4aae-829b-c3bad3227d9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Helper functions\n",
    "'''\n",
    "\n",
    "def display_image_with_metadata(img_tensor,\n",
    "                                state_one_hot,\n",
    "                                species_one_hot,\n",
    "                                ndvi,\n",
    "                                height,\n",
    "                                target_vec,\n",
    "                                base_dataset,\n",
    "                                ax=None):\n",
    "    \"\"\"\n",
    "    Show image plus state, species, NDVI, height, and target vector.\n",
    "    \"\"\"\n",
    "    IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "    IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "    IMAGENET_MEAN_T = torch.tensor(IMAGENET_MEAN).view(3, 1, 1)\n",
    "    IMAGENET_STD_T  = torch.tensor(IMAGENET_STD).view(3, 1, 1)\n",
    "\n",
    "    # Unnormalize image to [0, 1]\n",
    "    img = img_tensor * IMAGENET_STD_T + IMAGENET_MEAN_T\n",
    "    img_np = img.clamp(0, 1).permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    # Decode one-hot → labels\n",
    "    state_idx = state_one_hot.argmax().item()\n",
    "    species_idx = species_one_hot.argmax().item()\n",
    "\n",
    "    state_label = base_dataset.idx_to_state[state_idx]\n",
    "    species_label = base_dataset.idx_to_species[species_idx]\n",
    "\n",
    "    # Build title string\n",
    "    target_str = \", \".join(f\"{v:.3f}\" for v in target_vec.tolist())\n",
    "    title = (\n",
    "        f\"State: {state_label} | Species: {species_label}\\n\"\n",
    "        f\"Pre_GSHH_NDVI: {ndvi:.3f} | Height: {height:.2f} cm\\n\"\n",
    "        f\"Targets: [{target_str}]\"\n",
    "    )\n",
    "\n",
    "    if ax is None:\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.imshow(img_np)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(title, fontsize=8)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        ax.imshow(img_np)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(title, fontsize=8)\n",
    "        \n",
    "        \n",
    "# Helper function to visualize performance during training\n",
    "def plot_training_curves(train_losses):\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "    \n",
    "    ax.plot(train_losses)\n",
    "    ax.set_title('Training Loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.grid(True)\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83561207-5e74-4e96-b60c-cca826ae2780",
   "metadata": {},
   "source": [
    "# Model functionality demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd10cb8-8069-4c2f-a0bc-b39a8a1bf17b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Load in and organize data.\n",
    "Creates train, validation, and test dataloaders.\n",
    "'''\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Paths to data\n",
    "# -------------------------------------------------------------------\n",
    "CSV_PATH = \"../data/train.csv\"   # change if needed\n",
    "IMAGE_ROOT = \"../data\"           # root folder for images\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Create dataset and split into train / val / test\n",
    "# -------------------------------------------------------------------\n",
    "# First, create a \"base\" dataset (no transform) just to define splits\n",
    "base_dataset = PastureDataset(\n",
    "    csv_path=CSV_PATH,\n",
    "    image_root=IMAGE_ROOT,\n",
    "    transform=None\n",
    ")\n",
    "\n",
    "N = len(base_dataset)\n",
    "train_ratio = 0.7\n",
    "val_ratio   = 0.15\n",
    "test_ratio  = 0.15\n",
    "\n",
    "train_size = int(train_ratio * N)\n",
    "val_size   = int(val_ratio * N)\n",
    "test_size  = N - train_size - val_size   # ensures all samples used\n",
    "\n",
    "train_base, val_base, test_base = random_split(\n",
    "    base_dataset,\n",
    "    [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42),\n",
    ")\n",
    "\n",
    "#transforms for images\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "# image augmentation for train set\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(\n",
    "        size=128,\n",
    "        scale=(0.8, 1.0),\n",
    "        ratio=(0.9, 1.1),\n",
    "    ),\n",
    "\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "\n",
    "    transforms.ColorJitter(\n",
    "        brightness=0.25,\n",
    "        contrast=0.25,\n",
    "        saturation=0.25,\n",
    "    ),\n",
    "\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "    transforms.RandomErasing(p=0.2, scale=(0.02, 0.1)),\n",
    "\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# transform for validation and test (no strong augmentation)\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# Now create *separate* datasets with appropriate transforms\n",
    "full_train_dataset = PastureDataset(\n",
    "    csv_path=CSV_PATH,\n",
    "    image_root=IMAGE_ROOT,\n",
    "    transform=train_transform,\n",
    ")\n",
    "\n",
    "full_eval_dataset = PastureDataset(\n",
    "    csv_path=CSV_PATH,\n",
    "    image_root=IMAGE_ROOT,\n",
    "    transform=eval_transform,\n",
    ")\n",
    "\n",
    "# Use the same indices as the base splits\n",
    "train_dataset = Subset(full_train_dataset, train_base.indices)\n",
    "val_dataset   = Subset(full_eval_dataset,  val_base.indices)\n",
    "test_dataset  = Subset(full_eval_dataset,  test_base.indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=32, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Total samples: {N}\")\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Val samples:   {len(val_dataset)}\")\n",
    "print(f\"Test samples:  {len(test_dataset)}\")\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches:   {len(val_loader)}\")\n",
    "print(f\"Test batches:  {len(test_loader)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856b9ccb-ea2e-402f-8e1b-9a93c5c6bf72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Displays first 4 input data from test set and\n",
    "prints model predictions for all data in test set\n",
    "'''\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Visualize a batch from the test loader\n",
    "# -------------------------------------------------------------------\n",
    "batch = next(iter(test_loader))\n",
    "(\n",
    "    batch_imgs,\n",
    "    batch_state_oh,\n",
    "    batch_species_oh,\n",
    "    batch_ndvi,\n",
    "    batch_height,\n",
    "    batch_targets,\n",
    ") = batch\n",
    "\n",
    "# random_split returns Subset, so base_dataset is the underlying PastureDataset\n",
    "dem_dataset = test_dataset.dataset\n",
    "\n",
    "# Show first few images + metadata\n",
    "num_to_show = min(4, batch_imgs.size(0))\n",
    "fig, axes = plt.subplots(1, num_to_show, figsize=(4 * num_to_show, 4))\n",
    "\n",
    "if num_to_show == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i in range(num_to_show):\n",
    "    display_image_with_metadata(\n",
    "        img_tensor=batch_imgs[i],\n",
    "        state_one_hot=batch_state_oh[i],\n",
    "        species_one_hot=batch_species_oh[i],\n",
    "        ndvi=batch_ndvi[i].item(),\n",
    "        height=batch_height[i].item(),\n",
    "        target_vec=batch_targets[i],\n",
    "        base_dataset=base_dataset,\n",
    "        ax=axes[i],\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Run model on batch and print results\n",
    "#-----------------------------------------------------------------------------\n",
    "model = BiomassModel()\n",
    "model.load_state_dict(torch.load(\"../models/model_weights.pth\", map_location=\"cpu\"))\n",
    "\n",
    "images, states, species, ndvis, heights, targets = batch\n",
    "preds = model(images, states, species, ndvis, heights)\n",
    "print(\"Predictions: \", preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36362f98-b9e4-4ad9-8dfb-ade2801529f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Script for training model\n",
    "\n",
    "Commented out to prevent accidental retraining \n",
    "'''\n",
    "\n",
    "'''\n",
    "model = BiomassModel(drop_percent=0.3)\n",
    "train_model(model, train_loader, val_loader, final_layer_only=True)\n",
    "train_model(model, train_loader, val_loader, final_layer_only=False)\n",
    "#torch.save(model.state_dict(), \"model_weights.pth\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46850f5e-56f6-450b-b7df-97fc1f2e5304",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Script for evaluating model loss on test set\n",
    "'''\n",
    "\n",
    "model = BiomassModel()\n",
    "model.load_state_dict(torch.load(\"../models/model_weights.pth\", map_location=\"cpu\"))\n",
    "test(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8125ea26-e047-48e5-9314-75e67fcc3666",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
